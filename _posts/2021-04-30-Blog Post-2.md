---
layout: post
title: Blog Post 2 Spectral Clustering
---

In this blog post, you'll write a tutorial on a simple version of the *spectral clustering* algorithm for clustering data points. Each of the below parts will pose to you one or more specific tasks. You should plan to both:

- Achieve these tasks using clean, efficient, and well-documented Python and 
- Write, in your own words, about how to understand what's going on.  

> Remember, your aim is not just to write and understand the algorithm, but to explain to someone else how they could do the same. 

***Note***: your blog post doesn't have to contain a lot of math. It's ok for you to give explanations like "this function is an approximation of this other function according to the math in the assignment." 

### Notation

In all the math below: 

- Boldface capital letters like $$\mathbf{A}$$ refer to matrices (2d arrays of numbers). 
- Boldface lowercase letters like $$\mathbf{v}$$ refer to vectors (1d arrays of numbers). 
- $$\mathbf{A}\mathbf{B}$$ refers to a matrix-matrix product (`A@B`). $$\mathbf{A}\mathbf{v}$$ refers to a matrix-vector product (`A@v`). 

### Comments and Docstrings

You should plan to comment all of your code. Docstrings are not required except in Part G. 

## Introduction

In this problem, we'll study *spectral clustering*. Spectral clustering is an important tool for identifying meaningful parts of data sets with complex structure. To start, let's look at an example where we *don't* need spectral clustering. 


```python
# importing packages that will be necessary throughout the blog post
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```

We observe that the data below is pretty well clustered on its own. This implies that we wouldn't need fancy clustering algorithms to separate the data into their own clusters. Instead, we can use an algorithm like **KMeans** to do the work for us!


```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x1220e88d0>




    
![output_3_1.png](/images/output_3_1.png)
    


*Clustering* refers to the task of this data set into the two natural "blobs." K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these: 


```python
# importing KMeans package
from sklearn.cluster import KMeans

# creating a KMeans model with two clusters
km = KMeans(n_clusters = 2)

# fitting the KMeans model to the data
km.fit(X)

# creating a scatterplot that shows how KMeans clusters the data 
plt.scatter(X[:,0], X[:,1], c = km.predict(X));
```


    
![output_5_0.png](/images/output_5_0.png)
    


### Harder Clustering

That was all well and good, but what if our data is "shaped weird"? 


```python
# setting the seed so that we get similar data everytime
np.random.seed(1234)
n = 200

# creating fake data that does not have definitive clusters
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)

# plotting the data
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x122e80d50>




    
![output_7_1.png](/images/output_7_1.png)
    


We can still make out two meaningful clusters in the data, but now they aren't blobs but crescents. As before, the Euclidean coordinates of the data points are contained in the matrix `X`, while the labels of each point are contained in `y`. Now k-means won't work so well, because k-means is, by design, looking for circular clusters. 


```python
# running KMeans on these clusters 
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X));
```


    
![output_9_0.png](/images/output_9_0.png)
    


Whoops! That's not right! 

As we'll see, spectral clustering is able to correctly cluster the two crescents. In the following problems, you will derive and implement spectral clustering. 

## Part A

Construct the *similarity matrix* $$\mathbf{A}$$. $$\mathbf{A}$$ should be a matrix (2d `np.ndarray`) with shape `(n, n)` (recall that `n` is the number of data points). 

When constructing the similarity matrix, use a parameter `epsilon`. Entry `A[i,j]` should be equal to `1` if `X[i]` (the coordinates of data point `i`) is within distance `epsilon` of `X[j]` (the coordinates of data point `j`). 

**The diagonal entries `A[i,i]` should all be equal to zero.** The function `np.fill_diagonal()` is a good way to set the values of the diagonal of a matrix.  

#### Note

It is possible to do this manually in a `for`-loop, by testing whether `(X[i] - X[j])**2 < epsilon**2` for each choice of `i` and `j`. This is not recommended! Instead, see if you can find a solution built into `sklearn`. Can you find a function that will compute all the pairwise distances and collect them into an appropriate matrix for you? 

For this part, use `epsilon = 0.4`. 

In the first part of our spectral clustering algorithm, we will create the similarity matrix which contains information on the pairwise distances between the points in the matrix X. If the points are within **epsilon** distance of each other, then we will label those points as `0` and if not then they will be a `1`. Hence the similarity matrix will be comprised of zeros and ones. 


```python
# import the pairwise_distances function from sklearn which computes the distance matrix from all points in the matrix X
from sklearn.metrics.pairwise import pairwise_distances

def similarity_matrix(X,epsilon):
    
    # defining a matrix A which will contain the distances between the points in the input matrix
    A = pairwise_distances(X, metric = 'euclidean')
    
    # if points are within a distance epsilon, then we mark those as a 1, else then the points are labeled a 0
    A = (A < epsilon).astype(int)
    
    # filling the diagonal with 0
    np.fill_diagonal(A, 0)
    
    # returning similarity matrix
    return A

# creating the similarity matrix through a function call
A = similarity_matrix(X,0.4) 
A

```




    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 1, 0],
           ...,
           [0, 0, 0, ..., 0, 1, 1],
           [0, 0, 1, ..., 1, 0, 1],
           [0, 0, 0, ..., 1, 1, 0]])



## Part B

The matrix `A` now contains information about which points are near (within distance `epsilon`) which other points. We now pose the task of clustering the data points in `X` as the task of partitioning the rows and columns of `A`. 

Let $$d_i = \sum_{j = 1}^n a_{ij}$$ be the $$i$$th row-sum of $$\mathbf{A}$$, which is also called the *degree* of $$i$$. Let $$C_0$$ and $$C_1$$ be two clusters of the data points. We assume that every data point is in either $$C_0$$ or $$C_1$$. The cluster membership as being specified by `y`. We think of `y[i]` as being the label of point `i`. So, if `y[i] = 1`, then point `i` (and therefore row $$i$$ of $$\mathbf{A}$$) is an element of cluster $$C_1$$.  

The *binary norm cut objective* of a matrix $$\mathbf{A}$$ is the function 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In this expression, 
- $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ is the *cut* of the clusters $$C_0$$ and $$C_1$$. 
- $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row $$i$$ (the total number of all other rows related to row $$i$$ through $$A$$). The *volume* of cluster $$C_0$$ is a measure of the size of the cluster. 

A pair of clusters $$C_0$$ and $$C_1$$ is considered to be a "good" partition of the data when $$N_{\mathbf{A}}(C_0, C_1)$$ is small. To see why, let's look at each of the two factors in this objective function separately. 


#### B.1 The Cut Term

First, the cut term $$\mathbf{cut}(C_0, C_1)$$ is the number of nonzero entries in $$\mathbf{A}$$ that relate points in cluster $$C_0$$ to points in cluster $$C_1$$. Saying that this term should be small is the same as saying that points in $$C_0$$ shouldn't usually be very close to points in $$C_1$$. 

Write a function called `cut(A,y)` to compute the cut term. You can compute it by summing up the entries `A[i,j]` for each pair of points `(i,j)` in different clusters. 

It's ok if you use `for`-loops in this function -- we are going to see a more efficient view of this problem soon. 


```python
def cut(A,y):
    
    # c represents the cut value which will be computed within this function
    c = 0
    
    # double for loop which loops through all rows and columns of A
    for i in range(0,y.shape[0]): 
        for j in range(0,y.shape[0]):
            
            # checks if a pair of points (i,j) are in different clusters (if so then we add the respective entry in the matrix A to the cut value c)
            if (y[i] != y[j]):
                c += A[i,j]
                
    # returning the cut value           
    return c

# outputing the value computed from our cut function
cut(A,y)
    

```




    26



Compute the cut objective for the true clusters `y`. Then, generate a random vector of random labels of length `n`, with each label equal to either 0 or 1. Check the cut objective for the random labels. You should find that the cut objective for the true labels is *much* smaller than the cut objective for the random labels. 

This shows that this part of the cut objective indeed favors the true clusters over the random ones. 


```python
# cut_true represents the cut objective value for the true clusters y
cut_true = cut(A,y)

# creating a random vector of size n that contains 0 or 1
random_vector = np.random.randint(2,size = n)

# cut_random represents the cut objective value for the random labels
cut_random = cut(A, random_vector)

# comparing the cut objective for the true vs random labels
print("Cut True: ", cut_true, "\nCut Random:  ", cut_random)
```

    Cut True:  26 
    Cut Random:   2300



```python
d = A.sum(axis = 1)
d
```




    array([15, 25, 24, 27, 22, 25, 15, 19, 24, 21, 23, 24, 17, 25, 25, 24, 22,
           25, 26, 24, 24, 12, 21, 27, 21, 22, 22, 21, 24, 24, 21, 18, 25, 22,
           24, 23, 21, 24, 23, 24, 23, 25, 23, 14, 22, 27, 25, 25, 23, 14, 23,
           23, 21, 24, 19, 25, 24, 16, 21, 26, 15, 24, 23, 25, 22, 23, 15, 21,
           27, 22, 19, 23, 20, 26, 25, 30, 22, 25, 23, 23, 27, 23, 26, 24, 24,
           23, 22, 26, 23, 25, 17, 23, 24, 23, 16, 21, 24, 24, 22, 23, 24, 23,
           23, 14, 23, 25, 21, 26, 24, 25, 22, 26, 24, 24, 23, 20,  8, 15, 16,
           25, 26, 25, 25, 22, 19, 25, 24, 26, 18, 16, 23, 15, 26, 22, 23, 24,
           25, 22, 24, 11, 21, 23, 26, 25, 12, 26, 23, 26, 23, 24, 23, 23, 25,
           22, 25, 15, 17, 23, 26, 26, 25, 22, 23, 26, 17, 22, 25, 25, 15, 13,
           25, 25, 25, 25, 23, 24, 24, 26, 27, 22, 22, 25, 24, 23, 24, 23, 21,
           24, 24, 23, 24, 25, 21, 25, 24, 24, 24, 26, 28, 26])



#### B.2 The Volume Term 

Now take a look at the second factor in the norm cut objective. This is the *volume term*. As mentioned above, the *volume* of cluster $$C_0$$ is a measure of how "big" cluster $$C_0$$ is. If we choose cluster $$C_0$$ to be small, then $$\mathbf{vol}(C_0)$$ will be small and $$\frac{1}{\mathbf{vol}(C_0)}$$ will be large, leading to an undesirable higher objective value. 

Synthesizing, the binary normcut objective asks us to find clusters $$C_0$$ and $$C_1$$ such that:

1. There are relatively few entries of $$\mathbf{A}$$ that join $$C_0$$ and $$C_1$$. 
2. Neither $$C_0$$ and $$C_1$$ are too small. 

Write a function called `vols(A,y)` which computes the volumes of $$C_0$$ and $$C_1$$, returning them as a tuple. For example, `v0, v1 = vols(A,y)` should result in `v0` holding the volume of cluster `0` and `v1` holding the volume of cluster `1`. Then, write a function called `normcut(A,y)` which uses `cut(A,y)` and `vols(A,y)` to compute the binary normalized cut objective of a matrix `A` with clustering vector `y`. 

***Note***: No for-loops in this part. Each of these functions should be implemented in five lines or less. 


```python
def vols(A,y):
    
    # d is a n x 1 dimensional vector where each ith entry represents the ith row sum of A
    d = A.sum(axis = 1)
    
    # getting the entries in d that correspond to entries in the y vector that have a 1 and then summing all those entries
    v1 = sum(d[y == 1])
    
    # getting the entries in d that correspond to entries in the y vector that have a 0 and then summing all those entries
    v0 = sum(d[y==0])
    
    # returning both volumes for each cluster
    return v0,v1


def normcut(A,y):
    
    # getting the volume for each cluster by calling the function vols which we created above
    v0, v1 = vols(A,y)
    
    # returning the normcut objective value 
    return cut(A,y) * (1/v0 + 1/v1)

# displaying the normcut value
normcut(A,y)
    
```




    0.02303682466323045



Now, compare the `normcut` objective using both the true labels `y` and the fake labels you generated above. What do you observe about the normcut for the true labels when compared to the normcut for the fake labels? 

**We observe that for the true labels, the normcut value is much smaller in comparison to the fake labels by a factor of around 100.**


```python
# comparing the normcut value for true label vs fake labels
print("Normcut for true labels: ", normcut(A,y), "\nNormcut for fake labels: ", normcut(A,random_vector))
```

    Normcut for true labels:  0.02303682466323045 
    Normcut for fake labels:  2.0480047195518316


## Part C

We have now defined a normalized cut objective which takes small values when the input clusters are (a) joined by relatively few entries in $A$ and (b) not too small. One approach to clustering is to try to find a cluster vector `y` such that `normcut(A,y)` is small. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We need a math trick! 

Here's the trick: define a new vector $$\mathbf{z} \in \mathbb{R}^n$$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


Note that the signs of  the elements of $$\mathbf{z}$$ contain all the information from $$\mathbf{y}$$: if $$i$$ is in cluster $$C_0$$, then $$y_i = 0$$ and $$z_i > 0$$. 

Next, if you like linear algebra, you can show that 

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $$\mathbf{D}$$ is the diagonal matrix with nonzero entries $$d_{ii} = d_i$$, and  where $$d_i = \sum_{j = 1}^n a_i$$ is the degree (row-sum) from before.  

1. Write a function called `transform(A,y)` to compute the appropriate $$\mathbf{z}$$ vector given `A` and `y`, using the formula above. 
2. Then, check the equation above that relates the matrix product to the normcut objective, by computing each side separately and checking that they are equal. 
3. While you're here, also check the identity $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$, where $$\mathbb{1}$$ is the vector of `n` ones (i.e. `np.ones(n)`). This identity effectively says that $$\mathbf{z}$$ should contain roughly as many positive as negative entries. 

#### Programming Note

You can compute $$\mathbf{z}^T\mathbf{D}\mathbf{z}$$ as `z@D@z`, provided that you have constructed these objects correctly. 

#### Note

The equation above is exact, but computer arithmetic is not! `np.isclose(a,b)` is a good way to check if `a` is "close" to `b`, in the sense that they differ by less than the smallest amount that the computer is (by default) able to quantify. 

Also, still no for-loops. 


```python
def transform(A,y):
    
    # computing the volumes for each respective cluster using our vols function from above
    v0,v1 = vols(A,y)
    
    # creating
    #z = np.zeros((n,n))
    
    # creating a n x 1 vector of zeros
    z = np.zeros(n)
    
    # for entries in the vector y that have a 0, for that same entry in z we assign it to the volume of cluster 0
    z[y == 0] = v0
    
     # for entries in the vector y that have a 1, for that same entry in z we assign it to the volume of cluster 1
    z[y == 1] = -v1
    
    # taking the inverse of each element in z
    return np.array([1/z]).T
    
    
    
```


```python
# creating an n x n matrix D which contains all zeros
D = np.zeros((n,n))

# filling the diagonal matrix D which contains the ith row sum of A in the ith diagonal entry
np.fill_diagonal(D, A.sum(axis = 1))

# getting the z matrix from our transform function
z = transform(A,y)

# checking the equation above that relates the matrix product to the normcut objective by computing each side separately and showing they are equal.
np.isclose(normcut(A,y), 2 * (z.T @ (D - A) @ z)/(z.T @ D @ z))
```




    array([[ True]])



We observe above that we obtained a true value, so this means the normcut objective and the equation under **Part (C)** are equal. This shows that the work we have done so far has been correct! We now check that $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$ holds.


```python
# computing the expression in the above markdown cell
z.T @ D @ np.ones(n)
```




    array([0.])



## Part D

In the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. It's actually possible to bake this condition into the optimization, by substituting for $$\mathbf{z}$$ the orthogonal complement of $$\mathbf{z}$$ relative to $$\mathbf{D}\mathbf{1}$$. In the code below, I define an `orth_obj` function which handles this for you. 

Use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $$\mathbf{z}$$. Note that this computation might take a little while. Explicit optimization can be pretty slow! Give the minimizing vector a name `z_`. 


```python
def orth(u, v):
    return (u @ v) / (v @ v)*v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```


```python
# importing the minimize function from scipy.optimize
from scipy.optimize import minimize

# using the minimize function to minimize the function orth_obj with respect to z
z_ = minimize(orth_obj, z)
```


```python
z_
```




          fun: 0.005386460641590525
     hess_inv: array([[ 0.05470537,  0.05203258,  0.04795971, ...,  0.0591097 ,
             0.05700389,  0.04329117],
           [ 0.05203258,  0.90815553,  0.05158046, ...,  0.08988329,
             0.08992076,  0.0580807 ],
           [ 0.04795971,  0.05158046,  0.67225002, ...,  0.08428272,
             0.06728457,  0.05575329],
           ...,
           [ 0.0591097 ,  0.08988329,  0.08428272, ...,  0.75591721,
             0.01600954, -0.18296962],
           [ 0.05700389,  0.08992076,  0.06728457, ...,  0.01600954,
             0.19208848,  0.04070428],
           [ 0.04329117,  0.0580807 ,  0.05575329, ..., -0.18296962,
             0.04070428,  0.57338814]])
          jac: array([ 0.02793697, -0.001654  , -0.01905736,  0.11941124,  0.00564641,
           -0.04528222,  0.01674844, -0.00564105, -0.01858333, -0.00887884,
           -0.01748314,  0.00880352, -0.01191594, -0.01446844,  0.02426282,
           -0.00445451,  0.00155605, -0.00595018, -0.01845517,  0.00072717,
            0.01441362, -0.03617306,  0.02202246, -0.0094264 , -0.00523327,
            0.00803142, -0.00380893, -0.01847954,  0.0031331 , -0.02914287,
            0.01949445,  0.00506364, -0.0096039 , -0.02895674,  0.02322651,
            0.00468203, -0.00523326, -0.01188652, -0.02951975, -0.03595014,
            0.02672814,  0.007978  , -0.03303626,  0.04422454,  0.00539925,
           -0.02972501,  0.00846227,  0.01351109,  0.02730198, -0.0917218 ,
           -0.00164961, -0.01045153,  0.0220225 , -0.02534558, -0.01898904,
           -0.00980878, -0.01399556,  0.01583367, -0.04100848, -0.03296619,
            0.01847252,  0.02040917,  0.0085182 , -0.03984179,  0.01635294,
           -0.01573921,  0.01846657, -0.0154926 , -0.017753  ,  0.01149792,
            0.01950672,  0.00858827, -0.00744301,  0.01622191, -0.02482209,
            0.11697629,  0.01527739, -0.00991541, -0.01408969, -0.01620131,
            0.04267528,  0.00393023, -0.00264844, -0.01085805,  0.01030868,
           -0.02326566, -0.02895676, -0.01501412,  0.01394942, -0.07008199,
            0.02080776,  0.00851822,  0.01622236, -0.00164959,  0.01583096,
           -0.00887882, -0.01188651, -0.01486949, -0.00763013, -0.01045153,
           -0.00613948,  0.02730202, -0.00648875, -0.00071478, -0.00278785,
           -0.00958517,  0.03366067, -0.02152724, -0.01389002, -0.00015484,
            0.01540491,  0.01384783,  0.01802263,  0.00783919,  0.01591287,
            0.0620263 ,  0.00086425,  0.01677887,  0.01583737,  0.00148861,
           -0.00869011, -0.02325842,  0.02764726, -0.01023517,  0.00017928,
           -0.01905012, -0.00817321,  0.0165452 ,  0.00506347,  0.01583582,
           -0.01266384,  0.0167532 , -0.03296635, -0.03232018,  0.014231  ,
            0.00333119,  0.03165243, -0.0256706 ,  0.01483167, -0.0198354 ,
            0.02164752, -0.00941194, -0.04869922,  0.02842762, -0.00446729,
           -0.01210582,  0.00071931,  0.03354153, -0.02176107, -0.0141292 ,
           -0.03303625, -0.01266386, -0.0248221 ,  0.00803141,  0.02583121,
            0.01846424,  0.03439468, -0.01045153, -0.01210578, -0.00264851,
            0.00771358,  0.000301  ,  0.0041129 , -0.01501413,  0.01082538,
            0.0080314 , -0.00958516,  0.02842764,  0.01672937,  0.02365711,
           -0.00450579,  0.00689474, -0.00015483, -0.00259583, -0.02326567,
           -0.00732883,  0.01293565,  0.03354153,  0.03495983, -0.01023516,
            0.00155603, -0.00444957,  0.02040918, -0.00839616, -0.01858334,
           -0.00722322, -0.00047126,  0.02322647, -0.0139956 ,  0.02730202,
           -0.00732888,  0.00771357, -0.00047127,  0.01974496, -0.00419502,
           -0.00445446,  0.01140693, -0.04585083,  0.08616922, -0.03326733])
      message: 'Desired error not necessarily achieved due to precision loss.'
         nfev: 32975
          nit: 74
         njev: 164
       status: 2
      success: False
            x: array([-1.83351693e-03, -2.41690205e-03, -1.20207027e-03, -1.41735756e-03,
           -9.85466473e-04, -1.23070648e-03, -5.85569241e-04, -8.67361267e-04,
           -2.13343691e-03, -1.82257775e-03, -2.18391572e-03, -1.18162904e-03,
           -1.12559563e-03, -2.27553271e-03, -2.49824781e-03, -2.32658321e-03,
           -2.03851693e-03, -1.27295285e-03, -1.24203997e-03, -1.14268773e-03,
           -2.24017345e-03, -1.12821258e-03, -2.25923754e-03, -1.41876418e-03,
           -8.52699470e-04, -1.94361490e-03, -1.07863607e-03, -2.00268024e-03,
           -2.16739224e-03, -1.19993908e-03, -1.00730714e-03, -2.04768365e-03,
           -2.38474700e-03, -2.15570484e-03, -2.15326770e-03, -2.30799530e-03,
           -8.52699470e-04, -2.46809252e-03, -2.33945340e-03, -1.16151110e-03,
           -2.22234723e-03, -1.27153773e-03, -1.12803925e-03, -3.46734649e-04,
           -9.25708355e-04, -1.41498921e-03, -1.27221097e-03, -2.31458768e-03,
           -2.35146865e-03, -1.11518848e-03, -1.13536108e-03, -1.14943157e-03,
           -2.25923754e-03, -2.21216217e-03, -7.09310554e-04, -1.15725955e-03,
           -2.38234107e-03, -1.43637438e-03, -2.25724257e-03, -1.34583662e-03,
           -4.20014244e-04, -1.11073093e-03, -2.05774269e-03, -1.25835781e-03,
           -2.04507532e-03, -1.08269954e-03, -4.20014524e-04, -8.54192945e-04,
           -1.42121357e-03, -2.33015416e-03, -2.03774157e-03, -2.38878188e-03,
           -2.18974281e-03, -1.34279764e-03, -1.14313155e-03, -1.65957525e-03,
           -1.93543378e-03, -1.27217973e-03, -2.39826969e-03, -1.09175151e-03,
           -1.44521246e-03, -1.05087874e-03, -1.22144571e-03, -2.26372123e-03,
           -2.25728565e-03, -2.33457119e-03, -2.15570484e-03, -1.35411333e-03,
           -1.13209858e-03, -1.27190739e-03, -7.25633937e-04, -2.05774269e-03,
           -1.17074197e-03, -1.13536107e-03, -1.43637450e-03, -1.82257774e-03,
           -2.46809252e-03, -2.40293554e-03, -9.53779608e-04, -1.14943157e-03,
           -2.47003214e-03, -2.35146865e-03, -2.31339495e-03, -6.55748595e-04,
           -2.22974287e-03, -2.33788272e-03, -9.04811075e-04, -1.34553079e-03,
           -2.35500021e-03, -2.26919891e-03, -9.80959079e-04, -1.23204973e-03,
           -1.20404001e-03, -2.33181019e-03, -1.12883492e-03, -7.78292506e-04,
            8.24087110e-05, -5.85567809e-04, -1.43637421e-03, -2.38990427e-03,
           -2.60584738e-03, -1.20823261e-03, -2.22106970e-03, -2.30528801e-03,
           -2.11871351e-03, -1.15331634e-03, -2.37683198e-03, -1.35905000e-03,
           -2.04768366e-03, -1.43637428e-03, -1.00385195e-03, -5.85569017e-04,
           -1.34583663e-03, -1.07228427e-03, -2.38587960e-03, -2.47163239e-03,
           -2.32122210e-03, -2.07591644e-03, -2.22237870e-03, -1.55187707e-03,
           -1.84546729e-03, -2.37792074e-03, -1.21746204e-03, -1.27576496e-03,
           -1.62192934e-03, -1.21512153e-03, -1.03783607e-03, -1.23425800e-03,
           -1.12830638e-03, -1.20784200e-03, -1.12803925e-03, -1.00385195e-03,
           -1.14313155e-03, -1.94361490e-03, -2.50529581e-03, -4.20014634e-04,
           -1.97556897e-03, -1.14943157e-03, -1.21512153e-03, -1.22144572e-03,
           -2.42189069e-03, -2.24446472e-03, -1.08602196e-03, -1.35411333e-03,
           -1.51447681e-03, -1.94361490e-03, -2.33788272e-03, -1.27576496e-03,
           -5.85570139e-04, -4.45170623e-04, -2.53362448e-03, -1.16172388e-03,
           -2.26919891e-03, -2.32089476e-03, -2.33457119e-03, -1.20085978e-03,
           -1.20014214e-03, -1.23425800e-03, -1.41529496e-03, -2.30528801e-03,
           -2.03851693e-03, -1.15064182e-03, -1.11073093e-03, -1.99729715e-03,
           -2.13343691e-03, -2.05030238e-03, -9.86725906e-04, -2.15326770e-03,
           -2.38234107e-03, -2.35146865e-03, -1.20085978e-03, -2.42189069e-03,
           -9.86725906e-04, -2.52215764e-03, -2.27484961e-03, -2.32658321e-03,
           -2.09420907e-03, -1.33488174e-03, -1.49806929e-03, -1.33624818e-03])



**Note**: there's a cheat going on here! We originally specified that the entries of $$\mathbf{z}$$ should take only one of two values (back in Part C), whereas now we're allowing the entries to have *any* value! This means that we are no longer exactly optimizing the normcut objective, but rather an approximation. This cheat is so common that deserves a name: it is called the *continuous relaxation* of the normcut problem. 

## Part E

Recall that, by design, only the sign of `z_min[i]` actually contains information about the cluster label of data point `i`. Plot the original data, using one color for points such that `z_min[i] < 0` and another color for points such that `z_min[i] >= 0`. 

Does it look like we came close to correctly clustering the data? 


```python
# z_min is a numpy array and is the minimizer with respect to z
z_min = z_.x

# plotting the original data using green for points such that z_min[i] < -0.0015 and red otherwise
col = np.where(z_min < -0.0015,'green', 'red')
plt.scatter(X[:,0], X[:,1], c = col);


```


    
![output_34_0.png](/images/output_34_0.png)
    


We observe that our clustering algorithm has done a much better job of clustering the data points than was observed in the introduction when we applied KMeans to this same data. 

## Part F

Explicitly optimizing the orthogonal objective is  *way* too slow to be practical. If spectral clustering required that we do this each time, no one would use it. 

The reason that spectral clustering actually matters, and indeed the reason that spectral clustering is called *spectral* clustering, is that we can actually solve the problem from Part E using eigenvalues and eigenvectors of matrices. 

Recall that what we would like to do is minimize the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

with respect to $$\mathbf{z}$$, subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. 

The Rayleigh-Ritz Theorem states that the minimizing $$\mathbf{z}$$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

which is equivalent to the standard eigenvalue problem 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

Why is this helpful? Well, $$\mathbb{1}$$ is actually the eigenvector with smallest eigenvalue of the matrix $$\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$. 

> So, the vector $$\mathbf{z}$$ that we want must be the eigenvector with  the *second*-smallest eigenvalue. 

Construct the matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, which is often called the *Laplacian* matrix of the similarity matrix $$\mathbf{A}$$. Find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`. Then, plot the data again, using the sign of `z_eig` as the color. How did we do? 


```python
# constructing the Laplacian matrix of the similarity matrix A
L = np.linalg.inv(D) @ (D - A)

# unpacking the eigenvalues and eigenvectors from the matrix L
Lam,U = np.linalg.eig(L)

# sorting the eigenvalues
ix = Lam.argsort()

# sort by column
Lam, U = Lam[ix], U[:,ix]

# 2nd smallest eigenvalue and corresponding eigenvector
z_eig = U[:,1]
```


```python
# plotting the original data and using the sign of z_eig as the color
plt.scatter(X[:,0], X[:,1], c = z_eig);
```


    
![output_38_0.png](/images/output_38_0.png)
    


In fact, `z_eig` should be proportional to `z_min`, although this won't be exact because minimization has limited precision by default. 

## Part G

Synthesize your results from the previous parts. In particular, write a function called `spectral_clustering(X, epsilon)` which takes in the input data `X` (in the same format as Part A) and the distance threshold `epsilon` and performs spectral clustering, returning an array of binary labels indicating whether data point `i` is in group `0` or group `1`. Demonstrate your function using the supplied data from the beginning of the problem. 

#### Notes

Despite the fact that this has been a long journey, the final function should be quite short. You should definitely aim to keep your solution under 10, very compact lines. 

**In this part only, please supply an informative docstring!** 

#### Outline

Given data, you need to: 

1. Construct the similarity matrix. 
2. Construct the Laplacian matrix. 
3. Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix. 
4. Return labels based on this eigenvector. 


```python
def spectral_clustering(X, epsilon): 
    
    '''
    Takes in the input data X and the distance threshold epsilon and performs spectral clustering. 
    Returns an array of binary labels indicating whether data point i is in group 0 or group 1. The 
    function works in the following way:
    
    1. Constructs the similarity matrix
    2. Constructs the Laplacian matrix
    3. Computes the eigenvector with second-smallest eigenvalue of the Laplacian matrix 
    4. Returns labels based on this eigenvector
    
    '''
    
    # construct the similarity matrix using our function from Part (A)
    A = similarity_matrix(X,epsilon)
    
    D = np.zeros((X.shape[0], X.shape[0]))

    # filling the diagonal matrix D which contains the ith row sum of A in the ith diagonal entry
    np.fill_diagonal(D, A.sum(axis = 1))
    
    # construct the Laplacian matrix
    laplacian = np.linalg.inv(D) @ (D - A)
    
    # unpacking the eigenvalues and eigenvectors from the Laplacian
    Lam,U = np.linalg.eig(laplacian)
    
    # sorting the eigenvalues
    ix = Lam.argsort()


    # sort by column
    Lam, U = Lam[ix], U[:,ix]

    # getting the 2nd smallest eigenvalue and corresponding eigenvector
    z_eig = U[:,1]
    
    # creating an array which will contain the labels
    result = np.zeros(X.shape[0])
    
    # in entries where z_eig is greater than 0, assign those respective entries in result to 1
    result[z_eig > 0] = 1
    return result
    
    
```


```python
# plotting the data with the coloring specified by the specral clutering algorithm
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,epsilon = 0.4));
```


    
![output_42_0.png](/images/output_42_0.png)
    


After creating our spectral clustering function and plotting the data with the colors specified by our algorithm, we observe that the spectral clustering algorithm has much better success with regards to clustering the data than KMeans. This is the result we expected so we conclude that our clustering algorithm implementation worked!

## Part H

Run a few experiments using your function, by generating different data sets using `make_moons`. What happens when you increase the `noise`? Does spectral clustering still find the two half-moon clusters? For these experiments, you may find it useful to increase `n` to `1000` or so -- we can do this now, because of our fast algorithm! 


```python
# changing n value
n = 1000

# setting the seed so that our random data is consistent
np.random.seed(1111)

# making new moons data set with n_samples = 1000 and noise = 0.1
X, y = datasets.make_moons(n_samples = n, shuffle = True, noise = 0.1, random_state = None)

# plotting the data
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,epsilon = 0.4));
plt.title("n = 1000, noise = 0.1");
```


    
![output_45_0.png](/images/output_45_0.png)
    


**n = 1000, noise = 0.1**: We see that for more noise and data points, our spectral clustering algorithm still does a good job at finding the two-half moon clusters. There are a few points in the plot that might be wrongly classified, however for the most part our algorithm does a good job for this case.


```python
# setting the seed so that our random data is consistent
np.random.seed(1111)

# making new moons data set with n_samples = 1000 and noise = 0.2
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise = 0.2, random_state=None)

# plotting the data
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,epsilon = 0.4));
plt.title("n = 1000, noise = 0.2");
```


    
![output_47_0.png](/images/output_47_0.png)
    


**n = 1000, noise = 0.2**: For this case, the half moon shapes are a little more distorted due to the increase in the noise. However once again we see that our spectral clustering is successful in splitting the data into two clusters.


```python
# changing n value
n = 2000

# setting the seed so that our random data is consistent
np.random.seed(1111)

# making new moons data set with n_samples = 1000 and noise = 0.3
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise = 0.3, random_state=None)

# plotting the data
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,epsilon = 0.4));
plt.title("n = 200, noise = 0.3");
```


    
![output_49_0.png](/images/output_49_0.png)
    


**n = 2000, noise = 0.3**: A further increase in the noise along with a slight increase in the number of data points demonstrates that our clustering algorithm is still efficient in finding two clusters. 

## Part I

Now try your spectral clustering function on another data set -- the bull's eye! 


```python
n = 1000

# plotting different data set
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1]);
```


    
![output_52_0.png](/images/output_52_0.png)
    


There are two concentric circles. As before k-means will not do well here at all. 


```python
# fitting KMeans to this new data set
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X));
```


    
![output_54_0.png](/images/output_54_0.png)
    


Can your function successfully separate the two circles? Some experimentation here with the value of `epsilon` is likely to be required. Try values of `epsilon` between `0` and `1.0` and describe your findings. For roughly what values of `epsilon` are you able to correctly separate the two rings? 


```python
# executing clustering algorithm for epsilon = 0.2 on the new data set
np.random.seed(1111)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,epsilon = 0.2))
plt.title("Epsilon = 0.2");
```


    
![output_56_0.png](/images/output_56_0.png)
    


**epsilon = 0.2**: Our clustering algorithm classifies the inner circle as purple and the outer circle as yellow. This shows that our algorithm was able to distinguish between the two circles which is what we wanted to achieve! The purple circle on the inside means that the labels corresponding to `1` in the label matrix are within a distance epsilon while the labels corresponding to `0` are further than epsilon away.


```python
# executing clustering algorithm for epsilon = 0.4 on the new data set
np.random.seed(1111)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,epsilon = 0.4))
plt.title("Epsilon = 0.4");
```


    
![output_58_0.png](/images/output_58_0.png)
    


**epsilon = 0.4**: We observe the opposite result for this epsilon value compared to the previous case. For this epsilon value the labels have switched. The yellow circle on the inside means that the labels corresponding to `0` in the label matrix are within a distance epsilon while the labels corresponding to `1` are further than epsilon away. 


```python
# executing clustering algorithm for epsilon = 0.7 on the new data set
np.random.seed(1111)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,epsilon = 0.7))
plt.title("Epsilon = 0.7");
```


    
![output_60_0.png](/images/output_60_0.png)
    


**epsilon = 0.7**: For this epsilon value we see that the clustering algorithm does not do a good job of separating the different moons. This is because we have a greater tolerance of distance between the points, so points that are further apart might be classified in the same group. So we conclude that picking smaller epsilon values is better for separating the two moons.
 
